
import numpy as np


class GridWorld:
    def __init__(self):
        self.grid_size = 6
        self.terminal_states = [0, 35]
        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0, True

        
        row, col = divmod(state, self.grid_size)
        if action == 'UP':    row = max(row - 1, 0)
        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)
        elif action == 'LEFT':  col = max(col - 1, 0)
        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)

        next_state = row * self.grid_size + col
        reward = -1  
        done = next_state in self.terminal_states
        return next_state, reward, done

    def reset(self):
       
        start_state = np.random.randint(0, 16)
        while start_state in self.terminal_states:
            start_state = np.random.randint(0, 16)
        return start_state


def generate_episode(env):
    episode = []
    state = env.reset()
    done = False
    while not done:
        
        action = np.random.choice(env.actions)
        next_state, reward, done = env.step(state, action)
        episode.append((state, action, reward))
        state = next_state
    return episode


def mc_policy_evaluation(env, num_episodes=5000):
   
    V = np.zeros(env.grid_size * env.grid_size)

   
    returns = {s: [] for s in range(env.grid_size * env.grid_size)}

    for _ in range(num_episodes):
        episode = generate_episode(env)
        G = 0

       
        for idx in range(len(episode) - 1, -1, -1):
            state, action, reward = episode[idx]
            G = G + reward 

           
            previous_states = [x[0] for x in episode[:idx]]
            if state not in previous_states:
                returns[state].append(G)
                V[state] = np.mean(returns[state]) 
    return V


if __name__ == "__main__":
    env = GridWorld()
    print("Monte Carlo Policy Evaluation")

   
    values = mc_policy_evaluation(env)

    print("\nResulting Value Function (V):")
    
    print(np.round(values.reshape(6, 6), 1))
