
import numpy as np


class GridWorld:
    def __init__(self):
        self.grid_size = 4
        self.terminal_states = [0, 15] 
        self.actions = [0, 1, 2, 3] 

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0, True

        row, col = divmod(state, self.grid_size)

        # Move Logic
        if action == 0:   row = max(row - 1, 0) 
        elif action == 1: row = min(row + 1, self.grid_size - 1) 
        elif action == 2: col = max(col - 1, 0) 
        elif action == 3: col = min(col + 1, self.grid_size - 1) 

        next_state = row * self.grid_size + col
        reward = -1
        done = next_state in self.terminal_states
        return next_state, reward, done

    def reset(self):
        start_state = np.random.randint(0, 16)
        while start_state in self.terminal_states:
            start_state = np.random.randint(0, 16)
        return start_state


def q_learning():
    env = GridWorld()

    
    num_episodes = 5000
    alpha = 0.1   
    gamma = 1.0   
    epsilon = 0.1 

    
    Q = np.zeros((16, 4))

    print("Training with Q-Learning (5000 Episodes)...")

    for _ in range(num_episodes):
        state = env.reset()
        done = False

        while not done:
            
            if np.random.rand() < epsilon:
                action = np.random.choice(env.actions) 
            else:
                action = np.argmax(Q[state]) 

         
            next_state, reward, done = env.step(state, action)

            
            best_next_action_val = 0 if done else np.max(Q[next_state])

            
            td_target = reward + gamma * best_next_action_val
            Q[state, action] += alpha * (td_target - Q[state, action])

            state = next_state

    return Q


def print_policy(Q):
    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}
    print("\nFinal Optimal Policy (Q-Learning):")
    print("-" * 17)

    grid_output = []
    for s in range(16):
        if s in [0, 15]:
            grid_output.append(" T ")
            continue
       
        best_action = np.argmax(Q[s])
        grid_output.append(f" {actions_map[best_action]} ")

    for i in range(0, 16, 4):
        print("|".join(grid_output[i:i+4]))
        print("-" * 17)

if __name__ == "__main__":
    q_table = q_learning()
    print_policy(q_table)
