
import numpy as np

class MDPGridWorld:
    def __init__(self):
        self.grid_size = 4
        
        self.terminal_states = [15]
        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']
        self.gamma = 1.0 

    def get_next_state_reward(self, state, action):
        if state in self.terminal_states:
            return state, 0

        row, col = divmod(state, self.grid_size)

       
        if action == 'UP':    row = max(row - 1, 0)
        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)
        elif action == 'LEFT':  col = max(col - 1, 0)
        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)

        next_state = row * self.grid_size + col
        reward = -1 
        return next_state, reward

    def value_iteration(self):
        """1. PLAN: Calculate V* to find the best policy."""
        V = np.zeros(self.grid_size * self.grid_size)
        theta = 1e-4

        while True:
            delta = 0
            V_new = np.copy(V)
            for s in range(self.grid_size * self.grid_size):
                if s in self.terminal_states: continue

                action_values = []
                for a in self.actions:
                    ns, r = self.get_next_state_reward(s, a)
                    action_values.append(r + self.gamma * V[ns])

                new_val = max(action_values)
                V_new[s] = new_val
                delta = max(delta, abs(new_val - V[s]))
            V = V_new
            if delta < theta: break
        return V

    def rollout(self, start_state, V):
        """2. ACT: Execute the policy from the start state."""
        print(f"\n--- Rolling out Optimal Policy from State {start_state} (0,0) ---")

        curr_state = start_state
        steps = 0
        path = [curr_state]

        while curr_state not in self.terminal_states:
            row, col = divmod(curr_state, self.grid_size)

           
            best_action = None
            best_val = -float('inf')

            for action in self.actions:
                ns, r = self.get_next_state_reward(curr_state, action)
                val = r + self.gamma * V[ns]

                if val > best_val:
                    best_val = val
                    best_action = action

          
            next_state, _ = self.get_next_state_reward(curr_state, best_action)

            print(f"Step {steps+1}: At {curr_state} ({row},{col}) -> Action: {best_action} -> New State: {next_state}")

            curr_state = next_state
            path.append(curr_state)
            steps += 1

            if steps > 20: 
                print("Stuck in loop!")
                break

        print(f"\nGoal Reached at State {curr_state}!")
        print(f"Total Path: {path}")


if __name__ == "__main__":
    world = MDPGridWorld()

   
    print("Computing Optimal Policy...")
    optimal_values = world.value_iteration()

   
    world.rollout(start_state=0, V=optimal_values)
