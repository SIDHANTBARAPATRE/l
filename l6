
import numpy as np


class GridWorld:
    def __init__(self):
        self.grid_size = 4
        self.terminal_states = [0, 15]
        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0, True

        row, col = divmod(state, self.grid_size)

        if action == 'UP':    row = max(row - 1, 0)
        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)
        elif action == 'LEFT':  col = max(col - 1, 0)
        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)

        next_state = row * self.grid_size + col
        reward = -1
        done = next_state in self.terminal_states
        return next_state, reward, done

    def reset(self):
        start_state = np.random.randint(0, 16)
        while start_state in self.terminal_states:
            start_state = np.random.randint(0, 16)
        return start_state


def td_lambda_learning(env, lam=0.5, num_episodes=5000, alpha=0.1, gamma=1.0):
   
    V = np.zeros(env.grid_size * env.grid_size)

    print(f"Running TD({lam}) Learning...")

    for episode in range(num_episodes):
       
        E = np.zeros(env.grid_size * env.grid_size)

        state = env.reset()
        done = False

        while not done:
        
            action = np.random.choice(env.actions)
            next_state, reward, done = env.step(state, action)

           
            target = reward + (0 if done else gamma * V[next_state])
            delta = target - V[state]

           
            E[state] += 1

            V += alpha * delta * E

            E *= gamma * lam

            state = next_state

    return V


if __name__ == "__main__":
    env = GridWorld()

    
    v_values = td_lambda_learning(env, lam=0.5)

    print(f"\nState-Value Function V(s) via TD(0.5):")
    print("-" * 30)
    print(np.round(v_values.reshape(4, 4), 2))
