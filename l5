
import numpy as np


class GridWorld:
    def __init__(self):
        self.grid_size = 4
        self.terminal_states = [0, 15] 
        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0, True

        row, col = divmod(state, self.grid_size)

        if action == 'UP':    row = max(row - 1, 0)
        elif action == 'DOWN':  row = min(row + 1, self.grid_size - 1)
        elif action == 'LEFT':  col = max(col - 1, 0)
        elif action == 'RIGHT': col = min(col + 1, self.grid_size - 1)

        next_state = row * self.grid_size + col
        reward = -1
        done = next_state in self.terminal_states
        return next_state, reward, done

    def reset(self):
        start_state = np.random.randint(0, 16)
        while start_state in self.terminal_states:
            start_state = np.random.randint(0, 16)
        return start_state


def td_zero_learning(env, num_episodes=5000, alpha=0.1, gamma=1.0):
   
    V = np.zeros(env.grid_size * env.grid_size)

    print(f"Running TD(0) Learning for {num_episodes} episodes...")

    for _ in range(num_episodes):
        state = env.reset()
        done = False

        while not done:
            
            action = np.random.choice(env.actions)

            
            next_state, reward, done = env.step(state, action)

            current_value = V[state]

            
            next_state_value = 0 if done else V[next_state]
            td_target = reward + gamma * next_state_value

            
            V[state] = current_value + alpha * (td_target - current_value)

            state = next_state

    return V


if __name__ == "__main__":
    env = GridWorld()

    
    v_values = td_zero_learning(env)

    print("\nState-Value Function V(s) learned via TD(0):")
    print("-" * 30)

    
    print(np.round(v_values.reshape(4, 4), 2))
