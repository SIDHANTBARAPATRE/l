
import numpy as np


class GridWorld:
    def __init__(self):
        self.grid_size = 4
        self.terminal_states = [0, 15] 
        self.actions = [0, 1, 2, 3] 

    def step(self, state, action):
        if state in self.terminal_states:
            return state, 0, True

        row, col = divmod(state, self.grid_size)

        
        if action == 0:   row = max(row - 1, 0) 
        elif action == 1: row = min(row + 1, self.grid_size - 1) 
        elif action == 2: col = max(col - 1, 0) 
        elif action == 3: col = min(col + 1, self.grid_size - 1) 

        next_state = row * self.grid_size + col
        reward = -1 
        done = next_state in self.terminal_states

        return next_state, reward, done

    def reset(self):
        start_state = np.random.randint(0, 16)
        while start_state in self.terminal_states:
            start_state = np.random.randint(0, 16)
        return start_state


def q_learning():
    env = GridWorld()

    
    num_episodes = 5000
    alpha = 0.1   
    gamma = 0.99  
    epsilon = 0.1 

    
    
    Q = np.zeros((16, 4))

    print("Training with Q-Learning (5000 Episodes)...")

    for episode in range(num_episodes):
        state = env.reset()
        done = False

        while not done:
           
            if np.random.rand() < epsilon:
                action = np.random.choice(env.actions) 
            else:
                action = np.argmax(Q[state]) 

           
            next_state, reward, done = env.step(state, action)

            
           
            old_value = Q[state, action]
            
            next_max = np.max(Q[next_state])

            
            new_value = old_value + alpha * (reward + gamma * next_max - old_value)

            Q[state, action] = new_value

            state = next_state

    return Q


def print_results(Q):
    actions_map = {0: '↑', 1: '↓', 2: '←', 3: '→'}
    print("\nLearned Policy (from Q-Table):")
    print("-" * 17)

    grid_output = []
    for s in range(16):
        if s in [0, 15]:
            grid_output.append(" T ")
            continue

        
        best_action_idx = np.argmax(Q[s])
        grid_output.append(f" {actions_map[best_action_idx]} ")

    
    for i in range(0, 16, 4):
        print("|".join(grid_output[i:i+4]))
        print("-" * 17)

    print("\nExample Q-Values for State 1 (Next to Top-Left Goal):")
    print(f"UP: {Q[1,0]:.2f}, DOWN: {Q[1,1]:.2f}, LEFT: {Q[1,2]:.2f}, RIGHT: {Q[1,3]:.2f}")
    print("(Notice 'LEFT' should have the highest value because it leads to the goal)")

if __name__ == "__main__":
    final_Q = q_learning()
    print_results(final_Q)
