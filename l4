# lab4
import pandas as pd
import kagglehub
import numpy as np
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix
from fairlearn.metrics import (
    MetricFrame,
    selection_rate,
    true_positive_rate
)

path = kagglehub.dataset_download("spscientist/students-performance-in-exams")
dataset_file = os.path.join(path, 'StudentsPerformance.csv')
df = pd.read_csv(dataset_file)



label_enc = LabelEncoder()
for col in df.select_dtypes(include="object").columns:
    df[col] = label_enc.fit_transform(df[col])


df['passed_math'] = (df['math score'] >= 50).astype(int)


sensitive_feature = 'gender'

X = df.drop(columns=['math score', 'passed_math'])
y = df['passed_math']


X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)


model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)


acc = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {acc:.3f}")

mf = MetricFrame(
    metrics={
        'accuracy': accuracy_score,
        'selection_rate': selection_rate,
        'true_positive_rate': true_positive_rate,
    },
    y_true=y_test,
    y_pred=y_pred,
    sensitive_features=df.loc[y_test.index, sensitive_feature]
)

print("\n=== Fairness Metrics by Sensitive Group ===")
print(mf.by_group)


eo_diff = mf.difference(method='between_groups')['true_positive_rate']
print(f"\nEqual Opportunity Difference: {eo_diff:.3f}")


sr = mf.by_group['selection_rate']
disparate_impact = sr.min() / sr.max()
print(f"Disparate Impact (min/max selection rate): {disparate_impact:.3f}")


precision = {}
for g in df[sensitive_feature].unique():
    y_true_g = y_test[df.loc[y_test.index, sensitive_feature] == g]
    y_pred_g = y_pred[df.loc[y_test.index, sensitive_feature] == g]
    tn, fp, fn, tp = confusion_matrix(y_true_g, y_pred_g).ravel()
    precision[g] = tp / (tp + fp + 1e-9)
print(f"\nPredictive Parity by Group: {precision}")

print("\n=== Threshold Fairness Check ===")
y_prob = model.predict_proba(X_test)[:, 1]
thresholds = np.arange(0.1, 1.0, 0.1)
for t in thresholds:
    y_pred_t = (y_prob >= t).astype(int)
    tpr = true_positive_rate(y_test, y_pred_t)
    print(f"Threshold {t:.1f}: True Positive Rate = {tpr:.3f}")
